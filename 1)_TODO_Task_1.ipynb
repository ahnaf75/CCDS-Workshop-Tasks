{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTy1iEs2d42S"
      },
      "source": [
        "#### **Import the required libraries**\n",
        "\n",
        "We will be using a few libraries in this session, import all the required libraries in the cell below one by one as we go through the session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp2b4VPPd42U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import tqdm\n",
        "import yaml\n",
        "from sklearn.metrics import (ConfusionMatrixDisplay, classification_report,\n",
        "                             confusion_matrix)\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlYwVr9Ad42V"
      },
      "source": [
        "#### **Download the dataset**\n",
        "\n",
        "We will be using `CIFAR-10` dataset for this session. The dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. Please download the dataset using the following command.\n",
        "\n",
        "```python\n",
        "!wget https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/CIFAR-10.zip\n",
        "!wget https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/configs/CNN_config.yaml\n",
        "!unzip -q /content/CIFAR-10.zip -d /content/\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uF0CRzmd42W",
        "outputId": "0a2e967e-0818-4dc5-f80b-de5f02846714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-22 06:58:43--  https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/CIFAR-10.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ccdsiub/deeplearning-code-management/1.1/CIFAR-10.zip [following]\n",
            "--2024-12-22 06:58:44--  https://raw.githubusercontent.com/ccdsiub/deeplearning-code-management/1.1/CIFAR-10.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 57166560 (55M) [application/zip]\n",
            "Saving to: ‘CIFAR-10.zip’\n",
            "\n",
            "CIFAR-10.zip        100%[===================>]  54.52M   305MB/s    in 0.2s    \n",
            "\n",
            "2024-12-22 06:58:45 (305 MB/s) - ‘CIFAR-10.zip’ saved [57166560/57166560]\n",
            "\n",
            "--2024-12-22 06:58:45--  https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/configs/CNN_config.yaml\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/ccdsiub/deeplearning-code-management/1.1/configs/CNN_config.yaml [following]\n",
            "--2024-12-22 06:58:46--  https://raw.githubusercontent.com/ccdsiub/deeplearning-code-management/1.1/configs/CNN_config.yaml\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 197 [text/plain]\n",
            "Saving to: ‘CNN_config.yaml’\n",
            "\n",
            "CNN_config.yaml     100%[===================>]     197  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-22 06:58:46 (11.7 MB/s) - ‘CNN_config.yaml’ saved [197/197]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/CIFAR-10.zip\n",
        "!wget https://github.com/ccdsiub/deeplearning-code-management/raw/1.1/configs/CNN_config.yaml\n",
        "!unzip -q CIFAR-10.zip -d ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3bVtdjHd42U"
      },
      "source": [
        "#### **Load configuration file**\n",
        "\n",
        "We will be using a configuration file to store all the required parameters for the model. The configuration file is a YAML file, which is a human-readable data serialization standard that can be used in conjunction with all programming languages and is often used to write configuration files. Create a new file named `config.yaml` and add the following content to it.\n",
        "\n",
        "```yaml\n",
        "data_params:\n",
        "  data_path: \"CIFAR-10/\"\n",
        "```\n",
        "\n",
        "Read the configuration file using the `yaml` library and store the data in a variable named `config`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck0AjZy-d42V",
        "outputId": "cff77a93-9c3a-4a1d-aecf-7474fb58c8d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data_params': {'data_path': 'CIFAR-10/', 'train_ratio': 0.8, 'batch_size': 64}, 'model_params': {'num_classes': 10, 'learning_rate': 0.001, 'weight_decay': 0.0001, 'gamma': 0.1, 'step_size': 5, 'num_epochs': 10}}\n"
          ]
        }
      ],
      "source": [
        "file = open(\"CNN_config.yaml\")\n",
        "config = yaml.safe_load(file)\n",
        "file.close()\n",
        "print(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNgBOHn_d42W"
      },
      "source": [
        "#### **Load the dataset**\n",
        "\n",
        "Check the files in the side panel to see if the dataset is downloaded successfully. The dataset is organized in the following way.\n",
        "\n",
        "```\n",
        "CIFAR-10\n",
        " |\n",
        " └───airplane\n",
        " |\n",
        " └───automobile\n",
        " |\n",
        " └───bird\n",
        " |\n",
        " └───cat\n",
        " |\n",
        " └───deer\n",
        " |\n",
        " └───dog\n",
        " |\n",
        " └───frog\n",
        " |\n",
        " └───horse\n",
        " |\n",
        " └───ship\n",
        " |\n",
        " └───truck\n",
        "```\n",
        "\n",
        "Write a class named `CIFAR10Dataset` to load the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaso9Btsd42d"
      },
      "outputs": [],
      "source": [
        "class CIFAR10(torch.utils.data.Dataset):\n",
        "    \"\"\"The CIFAR10 dataset\"\"\"\n",
        "\n",
        "    def __init__(self, root, transform=None, target_transform=None) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the CIFAR10 dataset\n",
        "\n",
        "        :param root: The root directory of the dataset\n",
        "        :type root: str\n",
        "        :param transform: The transform to apply to the data\n",
        "        :type transform: callable\n",
        "        :param target_transform: The transform to apply to the target\n",
        "        :type target_transform: callable\n",
        "\n",
        "        :return: None\n",
        "        :rtype: None\n",
        "        \"\"\"\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.data = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._load_data()\n",
        "\n",
        "    def _load_data(self):\n",
        "        \"\"\"\n",
        "        Load the data from the root directory\n",
        "\n",
        "        :return: None\n",
        "        :rtype: None\n",
        "        \"\"\"\n",
        "        # Load the data\n",
        "        classes = os.listdir(self.root)\n",
        "        classes.sort()\n",
        "        for i, class_name in enumerate(classes):\n",
        "            class_dir = os.path.join(self.root, class_name)\n",
        "            for image_name in os.listdir(class_dir):\n",
        "                image_path = os.path.join(class_dir, image_name)\n",
        "                image = read_image(image_path)\n",
        "                self.data.append(image)\n",
        "                self.targets.append(float(i))\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"\n",
        "        Get the item at the given index\n",
        "\n",
        "        :param index: The index of the item\n",
        "        :type index: int\n",
        "\n",
        "        :return: The item at the given index\n",
        "        :rtype: tuple\n",
        "        \"\"\"\n",
        "        img, target = self.data[index], self.targets[index]\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        if self.target_transform is not None:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Get the length of the dataset\n",
        "\n",
        "        :return: The length of the dataset\n",
        "        :rtype: int\n",
        "        \"\"\"\n",
        "        return len(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tsE_OvVd42d"
      },
      "source": [
        "#### **Preprocess transformations**\n",
        "\n",
        "Insted of loading the entire dataset into memory, and then applying transformations, we will be using `torchvision.transforms` to apply transformations while loading the dataset. We will use the following transformations:\n",
        "\n",
        "1. `ToTensor`: Convert the image to a tensor with pixel values in the range [0, 1].\n",
        "2. `Normalize`: Normalize the tensor with mean and standard deviation.\n",
        "3. `Lambda`: To flatten the image tensor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaG4dLZbd42d"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ConvertImageDtype(torch.float32),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "])\n",
        "\n",
        "# you can transform the target too if you want (e.g. one hot encode)\n",
        "target_transform = transforms.Lambda(\n",
        "    lambda y: torch.tensor(y, dtype=torch.long))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEIRxQkkd42d"
      },
      "source": [
        "Initialize the `CIFAR10Dataset` class with the `data_path` from the configuration file or you can use `torchvision.Dataset.ImageFolder` to load the dataset. Pass the transformations to the `CIFAR10Dataset` class and load the dataset.\n",
        "You can load the class name from the folder using `os.listdir` for later use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8XafO6Wd42e"
      },
      "outputs": [],
      "source": [
        "data_path = config[\"data_params\"][\"data_path\"]\n",
        "classes = os.listdir(data_path)\n",
        "classes.sort()\n",
        "\n",
        "dataset = CIFAR10(data_path, transform=transform,\n",
        "                  target_transform=target_transform)\n",
        "\n",
        "# dataset = ImageFolder(data_path, transform=transform,\n",
        "#  target_transform=target_transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgm-T7Mgd42e"
      },
      "source": [
        "#### **Split the data**\n",
        "\n",
        "Use `torch.utils.data.random_split` to split the dataset into training and validation sets. Use 80% of the data for training and 10% for validation and 10% for testing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x1iLzb4d42e",
        "outputId": "7edae345-da2a-4f2c-95c6-cdcea610df59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train size: 48001\n",
            "Test size: 6000\n",
            "Validation size: 5999\n"
          ]
        }
      ],
      "source": [
        "train_size = config[\"data_params\"][\"train_ratio\"]\n",
        "test_size = (1 - train_size) / 2\n",
        "val_size = test_size\n",
        "\n",
        "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    dataset, [train_size, test_size, val_size])\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}\")\n",
        "print(f\"Test size: {len(test_dataset)}\")\n",
        "print(f\"Validation size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eawb00KXd42e"
      },
      "source": [
        "#### **Create data loaders**\n",
        "\n",
        "Use `torch.utils.data.DataLoader` to create data loaders for training, validation, and testing datasets. Use a batch size from the configuration file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7AjA87Wd42e"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config[\"data_params\"][\"batch_size\"],\n",
        "    num_workers=torch.get_num_threads(),\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config[\"data_params\"][\"batch_size\"],\n",
        "    num_workers=torch.get_num_threads(),\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config[\"data_params\"][\"batch_size\"],\n",
        "    num_workers=torch.get_num_threads(),\n",
        "    shuffle=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlTS_lQOd42e"
      },
      "source": [
        "#### **Multi-layer Perceptron (MLP) model**\n",
        "\n",
        "Create a class named `MLP` to define the model architecture. Now, we will create a simple MLP model with 3 layers. The input layer will have 32x32x3 neurons, the hidden layer will have 128 neurons, and the output layer will have 10 neurons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkOZtEMxd42e"
      },
      "outputs": [],
      "source": [
        "class Cifar10CnnModel(nn.Module):\n",
        "    \"\"\"The CIFAR10 CNN model\"\"\"\n",
        "\n",
        "    def __init__(self, output_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the CIFAR10 CNN model\n",
        "\n",
        "        :param output_size: The output size\n",
        "        :type output_size: int\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 64 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 128 x 8 x 8\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),  # output: 256 x 4 x 4\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(256*4*4, 1024),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Sigmoid(),\n",
        "            nn.Linear(512, output_size))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        :param x: The input data\n",
        "        :type x: torch.Tensor\n",
        "\n",
        "        :return: The output\n",
        "        :rtype: torch.Tensor\n",
        "        \"\"\"\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVaDvPhbd42f"
      },
      "source": [
        "#### **Initialize the model**\n",
        "\n",
        "Initialize the model and define the loss function, optimizer, and scheduler. Use the loss function as `nn.CrossEntropyLoss`, optimizer as `torch.optim.Adam`, and scheduler as `torch.optim.lr_scheduler.StepLR`. Use the learning rate and step size from the configuration file. Use the device as `cuda` if available, else use `cpu`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHGODD5Yd42f"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = Cifar10CnnModel(\n",
        "    config['model_params']['num_classes']\n",
        ").to(device)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    model.parameters(),\n",
        "    lr=config[\"model_params\"][\"learning_rate\"],\n",
        "    # L2 regularization\n",
        "    weight_decay=config[\"model_params\"][\"weight_decay\"]\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "    optimizer,\n",
        "    step_size=config[\"model_params\"][\"step_size\"],\n",
        "    gamma=config[\"model_params\"][\"gamma\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkSan24Bd42f"
      },
      "source": [
        "##### **Validation function**\n",
        "\n",
        "Create a function named `validate` to calculate the accuracy of the model on the validation dataset. Use the `model.eval` method to set the model to evaluation mode and use the `torch.no_grad` context manager to disable gradient calculation. Iterate through the validation data loader and calculate the accuracy of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CgLRWa5d42f"
      },
      "outputs": [],
      "source": [
        "def validate(\n",
        "    model,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    device: torch.device\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Validate the model\n",
        "\n",
        "    :param model: The model to validate\n",
        "    :type model: torch.nn.Module\n",
        "    :param val_loader: The validation loader\n",
        "    :type val_loader: torch.utils.data.DataLoader\n",
        "    :param criterion: The loss function\n",
        "    :type criterion: torch.nn.Module\n",
        "    :param device: The device to use\n",
        "    :type device: torch.device\n",
        "\n",
        "    :return: The loss, accuracy and predictions\n",
        "    :rtype: tuple\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    accuracy = correct / len(y_pred)\n",
        "\n",
        "    return val_loss, accuracy, y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhLskVOd42f"
      },
      "source": [
        "#### **Training function**\n",
        "\n",
        "Write a function named `train` to train the model. Use the `model.train` method to set the model to training mode. Iterate through the training data loader and calculate the loss and accuracy of the model. Use the `optimizer.zero_grad` method to zero the gradients and use the `loss.backward` method to backpropagate the loss. Use the `optimizer.step` method to update the model parameters. Use the `scheduler.step` method to update the learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8laHd23d42f"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    model: torch.nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler: torch.optim.lr_scheduler.StepLR,\n",
        "    device: torch.device,\n",
        "    num_epochs: int\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Train the model\n",
        "\n",
        "    :param model: The model to train\n",
        "    :type model: torch.nn.Module\n",
        "    :param train_loader: The training loader\n",
        "    :type train_loader: torch.utils.data.DataLoader\n",
        "    :param val_loader: The validation loader\n",
        "    :type val_loader: torch.utils.data.DataLoader\n",
        "    :param criterion: The loss function\n",
        "    :type criterion: torch.nn.Module\n",
        "    :param optimizer: The optimizer\n",
        "    :type optimizer: torch.optim.Optimizer\n",
        "    :param scheduler: The learning rate scheduler\n",
        "    :type scheduler: torch.optim.lr_scheduler.StepLR\n",
        "    :param device: The device to use\n",
        "    :type device: torch.device\n",
        "    :param num_epochs: The number of epochs\n",
        "    :type num_epochs: int\n",
        "\n",
        "    :return: The trained model and the training history\n",
        "    :rtype: tuple\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        loop = tqdm.tqdm(enumerate(train_loader),\n",
        "                         total=len(train_loader), leave=False)\n",
        "        for i, (images, labels) in loop:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "            loop.set_postfix(loss=train_loss / (i + 1))\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        val_loss, val_acc, _ = validate(model, val_loader, criterion, device)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        print(f\"Train loss: {train_loss:.4f}\")\n",
        "        print(f\"Validation loss: {val_loss:.4f}\")\n",
        "        print(f\"Validation accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            checkpoint = {\n",
        "                \"epoch\": epoch + 1,\n",
        "                \"model\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "                \"scheduler\": scheduler.state_dict(),\n",
        "                \"history\": history\n",
        "            }\n",
        "            torch.save(checkpoint, \"checkpoint.pth\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFPriGRxd42g"
      },
      "source": [
        "##### **Train the model**\n",
        "\n",
        "Call the `train` function to train the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7F-I14Ed42g",
        "outputId": "3c2faef3-d0fb-4e19-aff5-c7ccc681ccaf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10]\n",
            "Train loss: 2.2379\n",
            "Validation loss: 2.1498\n",
            "Validation accuracy: 0.20%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/10]\n",
            "Train loss: 2.1886\n",
            "Validation loss: 2.2984\n",
            "Validation accuracy: 0.10%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/10]\n",
            "Train loss: 2.2683\n",
            "Validation loss: 2.2080\n",
            "Validation accuracy: 0.17%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch [4/10]:  30%|███       | 228/751 [00:05<00:12, 41.11it/s, loss=2.28]"
          ]
        }
      ],
      "source": [
        "model, history = train(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    device,\n",
        "    num_epochs=config[\"model_params\"][\"num_epochs\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHqIlL22d42g"
      },
      "source": [
        "#### **Plot the learning curve**\n",
        "\n",
        "Plot the training loss and validation loss using `matplotlib`, keep both the training loss and validation loss in the same plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UhFH7S_d42g"
      },
      "outputs": [],
      "source": [
        "plt.plot(history[\"train_loss\"], label=\"Train loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Validation loss\")\n",
        "plt.title(\"Loss curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmYrwmXOd42g"
      },
      "source": [
        "#### **Load the model from the checkpoint**\n",
        "\n",
        "Use `torch.load` to load the model from the checkpoint and use the `model.load_state_dict` method to load the model weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHsvHeAxd42g"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load(\"checkpoint.pth\")[\"model\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcRgR9rBd42h"
      },
      "source": [
        "#### **Evaluate model performance**\n",
        "\n",
        "Now, we have trained the model. Call the `validate` function to validate the model and calculate the loss and predicted labels on the testing data.\n",
        "\n",
        "1. Use scikit-learn's `classification_report` function to evaluate the performance of the model. The `classification_report` function takes two inputs, `test_labels` and `predicted_labels`, and returns a report of the precision, recall, and F1-score of the model.\n",
        "2. Use the `confusion_matrix` function to get the confusion matrix of the model. It also takes two inputs, `test_labels` and `predicted_labels`, and returns the confusion matrix of the model. Use `ConfusionMatrixDisplay` to display the confusion matrix.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0KaFUhBd42h"
      },
      "outputs": [],
      "source": [
        "_, _, y_pred = validate(model, test_loader, criterion, device)\n",
        "y_test = [labels for _, labels in test_loader]\n",
        "y_test = torch.cat(y_test).numpy()\n",
        "\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(\n",
        "    y_test, y_pred), display_labels=classes)\n",
        "disp = disp.plot(xticks_rotation=45)\n",
        "plt.title('Confusion Matrix Test Data')\n",
        "plt.show()\n",
        "\n",
        "print('\\n\\n')\n",
        "print(classification_report(y_test, y_pred, target_names=classes))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}